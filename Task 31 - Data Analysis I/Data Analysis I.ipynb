{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis Task 1\n",
    "\n",
    "Data scientists spend a large amount of their time cleaning datasets and getting them down to a form with which they can work. In fact, a lot of data scientists argue that the initial steps of obtaining and cleaning data constitute 80% of the job. It is important to be able to deal with messy data, whether that means missing values, inconsistent formatting, malformed records, or nonsensical outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenjen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_table('balance.txt', delim_whitespace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns in a DataFrame\n",
    "\n",
    "Often, you’ll find that not all the categories of data in a dataset are useful to you. For example, you might have a dataset containing student information (name, grade, standard, parents’ names, and address) but want to focus on analyzing student grades.\n",
    "In this case, the address or parents’ names categories are not important to you. Retaining these unneeded categories will take up unnecessary space and potentially also bog down runtime.\n",
    "\n",
    "Pandas provides a handy way of removing unwanted columns or rows from a DataFramewith the drop() function. Let’s look at a simple example where we drop a number of columns from a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Limit','Age'],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we defined a list that contains the names of all the columns we want to drop. Next, we call the drop() function on our object, passing in the inplace parameter as True and the axis parameter as 1. This tells Pandas that we want the changes to be made directly in our object and that it should look for the values to be dropped in the columns of the object.\n",
    "\n",
    "When we inspect the DataFrame again, we’ll see that the unwanted columns have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balance</th>\n",
       "      <th>Income</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.240798</td>\n",
       "      <td>14.891</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.283334</td>\n",
       "      <td>106.025</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.530409</td>\n",
       "      <td>104.593</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.652811</td>\n",
       "      <td>148.924</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.893978</td>\n",
       "      <td>55.882</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Balance   Income  Rating  Cards  Education  Gender Student Married  \\\n",
       "0  12.240798   14.891     283      2         11    Male      No     Yes   \n",
       "1  23.283334  106.025     483      3         15  Female     Yes     Yes   \n",
       "2  22.530409  104.593     514      4         11    Male      No      No   \n",
       "3  27.652811  148.924     681      3         11  Female      No      No   \n",
       "4  16.893978   55.882     357      2         16    Male      No     Yes   \n",
       "\n",
       "   Ethnicity  \n",
       "0  Caucasian  \n",
       "1      Asian  \n",
       "2      Asian  \n",
       "3      Asian  \n",
       "4  Caucasian  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace values\n",
    "\n",
    "Sometimes you would like to replace a value from your data set with another value. For example if you had data with categories such as ‘Ethnicity’ and we wanted to rename one category lets say, 'African American' to 'African'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balance</th>\n",
       "      <th>Income</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.240798</td>\n",
       "      <td>14.891</td>\n",
       "      <td>283.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.283334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>483.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.530409</td>\n",
       "      <td>104.593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.652811</td>\n",
       "      <td>148.924</td>\n",
       "      <td>681.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.893978</td>\n",
       "      <td>55.882</td>\n",
       "      <td>357.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.486178</td>\n",
       "      <td>80.180</td>\n",
       "      <td>569.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Caucasian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.574516</td>\n",
       "      <td>20.996</td>\n",
       "      <td>259.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.576204</td>\n",
       "      <td>71.408</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.938090</td>\n",
       "      <td>15.125</td>\n",
       "      <td>266.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Caucasian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.756965</td>\n",
       "      <td>71.061</td>\n",
       "      <td>491.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Balance   Income  Rating  Cards  Education  Gender Student Married  \\\n",
       "0  12.240798   14.891   283.0    2.0       11.0    Male      No     Yes   \n",
       "1  23.283334      NaN   483.0    3.0       15.0  Female     NaN     Yes   \n",
       "2  22.530409  104.593     NaN    4.0       11.0    Male      No      No   \n",
       "3  27.652811  148.924   681.0    3.0       11.0     NaN      No      No   \n",
       "4  16.893978   55.882   357.0    2.0       16.0    Male      No     Yes   \n",
       "5  22.486178   80.180   569.0    4.0       10.0    Male      No      No   \n",
       "6  10.574516   20.996   259.0    2.0       12.0  Female      No      No   \n",
       "7  14.576204   71.408   512.0    2.0        9.0    Male      No      No   \n",
       "8   7.938090   15.125   266.0    5.0       13.0  Female      No      No   \n",
       "9  17.756965   71.061   491.0    3.0       19.0  Female     Yes     Yes   \n",
       "\n",
       "   Ethnicity  \n",
       "0  Caucasian  \n",
       "1      Asian  \n",
       "2      Asian  \n",
       "3      Asian  \n",
       "4  Caucasian  \n",
       "5  Caucasian  \n",
       "6    African  \n",
       "7      Asian  \n",
       "8  Caucasian  \n",
       "9    African  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.replace('African American','African').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Data\n",
    "\n",
    "Grouping data sets is a frequent need in data analysis where we need the result in terms of various groups present in the data set. Panadas has in-built methods which can roll the data into various groups.\n",
    "\n",
    "In the below example we group the data by Ethnicity and then get the result for a specific Ethnic group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balance</th>\n",
       "      <th>Income</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.283334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>483.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.530409</td>\n",
       "      <td>104.593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.652811</td>\n",
       "      <td>148.924</td>\n",
       "      <td>681.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.576204</td>\n",
       "      <td>71.408</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.218800</td>\n",
       "      <td>80.616</td>\n",
       "      <td>394.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Balance   Income  Rating  Cards  Education  Gender Student Married  \\\n",
       "1   23.283334      NaN   483.0    3.0       15.0  Female     NaN     Yes   \n",
       "2   22.530409  104.593     NaN    4.0       11.0    Male      No      No   \n",
       "3   27.652811  148.924   681.0    3.0       11.0     NaN      No      No   \n",
       "7   14.576204   71.408   512.0    2.0        9.0    Male      No      No   \n",
       "12  19.218800   80.616   394.0    1.0        7.0  Female      No     Yes   \n",
       "\n",
       "   Ethnicity  \n",
       "1      Asian  \n",
       "2      Asian  \n",
       "3      Asian  \n",
       "7      Asian  \n",
       "12     Asian  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby('Ethnicity')\n",
    "grouped.get_group('Asian').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with inconsistent data entry\n",
    "\n",
    "To begin with, let us install a module that will help us clean our data set. Go to your command prompt and type `pip3 install fuzzywuzzy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenjen/anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1ab86bc76cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# set seed for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# helpful modules\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "import chardet\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e21f1aeeace8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m suicide_attacks = pd.read_csv(\"Pakistan.csv\"\n\u001b[0m\u001b[1;32m      2\u001b[0m                               \u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'unicode_escape'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0msuicide_attacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "suicide_attacks = pd.read_csv(\"Pakistan.csv\"\n",
    "                              ,encoding = 'unicode_escape'\n",
    ")\n",
    "suicide_attacks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text pre-processing\n",
    "\n",
    "For this exercise, we are  interested in cleaning up the \"City\" column to make sure there's no data entry inconsistencies in it. We could go through and check each row by hand, of course, and hand-correct inconsistencies when we find them. There's a more efficient way to do this though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'suicide_attacks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-db1a01d6bf0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuicide_attacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'City'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# sort them alphabetically and then take a closer look\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'suicide_attacks' is not defined"
     ]
    }
   ],
   "source": [
    "cities = suicide_attacks['City'].unique()\n",
    "# sort them alphabetically and then take a closer look\n",
    "cities.sort()\n",
    "cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at this, I can see some problems due to inconsistent data entry. Let us look at the first entry 'ATTOCK' and 'Attock'. These are the same cities but the computer understands them as different.\n",
    "\n",
    "The first thing we need to do is make everything lower case (We can change it back at the end if we'd like) and remove any white spaces at the beginning and end of cells. Inconsistencies in capitalizations and trailing white spaces are very common in text data and you can fix a good 80% of your text data entry inconsistencies by doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'suicide_attacks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8155a2569983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# convert to lower case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msuicide_attacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'City'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuicide_attacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'City'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# remove trailing white spaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msuicide_attacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'City'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuicide_attacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'City'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'suicide_attacks' is not defined"
     ]
    }
   ],
   "source": [
    "# convert to lower case\n",
    "suicide_attacks['City'] = suicide_attacks['City'].str.lower()\n",
    "\n",
    "# remove trailing white spaces\n",
    "suicide_attacks['City'] = suicide_attacks['City'].str.strip()\n",
    "\n",
    "# Let us view the data\n",
    "\n",
    "cities = suicide_attacks['City'].unique()\n",
    "# sort them alphabetically and then take a closer look\n",
    "cities.sort()\n",
    "cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's take another look at the city column and see if there's any more data cleaning we need to do.\n",
    "\n",
    "It does look like there are some remaining inconsistencies: 'd. i khan' and 'd.i khan' should probably be the same.\n",
    "\n",
    "We are going to use the fuzzywuzzy package to help identify which string are closest to each other. This dataset is small enough that we could probably could correct errors by hand, but that approach doesn't scale well. (Would you want to correct a thousand errors by hand? What about ten thousand? Automating things as early as possible is generally a good idea. Plus, it’s fun! :)\n",
    "\n",
    "Fuzzy matching: The process of automatically finding text strings that are very similar to the target string. In general, a string is considered \"closer\" to another one the fewer characters you'd need to change if you were transforming one string into another. So \"apple\" and \"snapple\" are two changes away from each other (add \"s\" and \"n\") while \"in\" and \"on\" and one change away (rplace \"i\" with \"o\"). You won't always be able to rely on fuzzy matching 100%, but it will usually end up saving you at least a little time.\n",
    "\n",
    "Fuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we're going to get the ten strings from our list of cities that have the closest distance to \"d.i khan\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d. i khan', 100),\n",
       " ('d.i khan', 100),\n",
       " ('d.g khan', 88),\n",
       " ('khanewal', 50),\n",
       " ('sudhanoti', 47),\n",
       " ('hangu', 46),\n",
       " ('kohat', 46),\n",
       " ('dara adam khel', 45),\n",
       " ('chaman', 43),\n",
       " ('mardan', 43)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the top 10 closest matches to \"d.i khan\"\n",
    "matches = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "# take a look at them\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that two of the items in the cities are very close to \"d.i khan\": \"d. i khan\" and \"d.i khan\". We can also see the \"d.g khan\", which is a seperate city, has a ratio of 88. Since we don't want to replace \"d.g khan\" with \"d.i khan\", let's replace all rows in our City column that have a ratio of > 90 with \"d. i khan\".\n",
    "\n",
    "To do this, we going to write a function. (It's a good idea to write a general purpose function you can reuse if you think you might have to do a specific task more than once or twice. This keeps you from having to copy and paste code too often, which saves time and can help prevent mistakes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace rows in the provided column of the provided dataframe\n",
    "# that match the provided string above the provided ratio with the provided string\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n",
    "   \n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "    \n",
    "    # let us know the function's done\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function, we can put it to the test!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# use the function we just wrote to replace close matches to \n",
    "# \"d.i khan\" with \"d.i khan\"\n",
    "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"d.i khan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's can check the unique values in our City column again and make sure we've tidied up d.i khan correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['attock', 'bajaur agency', 'bannu', 'bhakkar', 'buner', 'chakwal',\n",
       "       'chaman', 'charsadda', 'd.g khan', 'd.i khan', 'dara adam khel',\n",
       "       'fateh jang', 'ghallanai, mohmand agency', 'gujrat', 'hangu',\n",
       "       'haripur', 'hayatabad', 'islamabad', 'jacobabad', 'karachi',\n",
       "       'karak', 'khanewal', 'khuzdar', 'khyber agency', 'kohat',\n",
       "       'kuram agency', 'kurram agency', 'lahore', 'lakki marwat',\n",
       "       'lasbela', 'lower dir', 'malakand', 'mansehra', 'mardan',\n",
       "       'mohmand agency', 'mosal kor, mohmand agency', 'multan',\n",
       "       'muzaffarabad', 'north waziristan', 'nowshehra', 'orakzai agency',\n",
       "       'peshawar', 'pishin', 'poonch', 'quetta', 'rawalpindi', 'sargodha',\n",
       "       'sehwan town', 'shabqadar-charsadda', 'shangla', 'shikarpur',\n",
       "       'sialkot', 'south waziristan', 'sudhanoti', 'sukkur', 'swabi',\n",
       "       'swat', 'taftan', 'tangi, charsadda district', 'tank', 'taunsa',\n",
       "       'tirah valley', 'totalai', 'upper dir', 'wagah', 'zhob'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the unique values in the 'City' column\n",
    "cities = suicide_attacks['City'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "cities.sort()\n",
    "cities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Data and time\n",
    "\n",
    "Analyzing datasets with dates and times is often very cumbersome. Months of different lengths, different distributions of weekdays and weekends, leap years, and the dreaded timezones are just a few things you may have to consider depending on your context. For this reason, Python has a data type specifically designed for dates and times called datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    November 19-1995\n",
      "1     November 6-2000\n",
      "2          May 8-2002\n",
      "3        June 14-2002\n",
      "4         July 4-2003\n",
      "Name: Date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# modules we'll use\n",
    "#import seaborn as sns\n",
    "from datetime import date\n",
    "# print the first few rows of the date column\n",
    "print(suicide_attacks['Date'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, those are dates! But just because we can understand that it doesnt meant that the computer understands them as so. Notice that the at the bottom of the output of head(), you can see that it says that the data type of this column is \"object\".\n",
    "\n",
    "Pandas uses the \"object\" dtype for storing various types of data types, but most often when you see a column with the dtype \"object\" it will have strings in it.\n",
    "\n",
    "If you check the pandas dtype documentation here, you'll notice that there's also a specific datetime64 dtypes. Because the dtype of our column is object rather than datetime64, we can tell that Python doesn't know that this column contains dates.\n",
    "\n",
    "We can also look at just the dtype of your column without printing the first few rows if we like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the data type of our date column\n",
    "suicide_attacks['Date'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to check the numpy documentation to match the letter code to the dtype of the object. \"O\" is the code for \"object\", so we can see that these two methods give us the same information.\n",
    "\n",
    "### Convert our date columns to datetime\n",
    "\n",
    "Now that we know that our date column isn't being recognized as a date, it's time to convert it so that it is recognized as a date. This is called \"parsing dates\" because we're taking in a string and identifying its component parts.\n",
    "\n",
    "We can pandas what the format of our dates are with a guide called as \"strftime directive\", which you can find more information on at this link. The basic idea is that you need to point out which parts of the date are where and what punctuation is between them. There are lots of possible parts of a date, \n",
    "\n",
    "\n",
    "|Code\t|Meaning\t|Example |\n",
    "|---------|-----------|---------------|\n",
    "|%A\t |   Weekday as locale’s full name |\tWednesday |\n",
    "|%a\t |   Weekday as locale’s abbreviated name |\tWed |\n",
    "|%B\t |  Month as locale’s full name |\tJune |\n",
    "|%d\t | Day of the month |\t06 |\n",
    "|%m\t |   Month as a number |\t6 |\n",
    "|%Y\t |  Four-digit year |\t2018 |\n",
    "|%y\t |  Two-digit year |\t18 |\n",
    "\n",
    "\n",
    "\n",
    "Some examples:\n",
    "\n",
    "1/17/07 has the format \"%m/%d/%y\"\n",
    "\n",
    "17-1-2007 has the format \"%d-%m-%Y\"\n",
    "\n",
    "\n",
    "Looking back up at the head of the date column in the landslides dataset, we can see that it's in the format \"month/day/two-digit year\", so we can use the same syntax as the first example to parse in our dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   1995-11-19\n",
       "1   2000-11-06\n",
       "2   2002-05-08\n",
       "3   2002-06-14\n",
       "4   2003-07-04\n",
       "Name: date_parsed, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new column, date_parsed, with the parsed dates\n",
    "\n",
    "\n",
    "suicide_attacks['date_parsed'] = pd.to_datetime(suicide_attacks['Date'], format='%B %d-%Y')\n",
    "\n",
    "\n",
    "suicide_attacks['date_parsed'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when I check the first few rows of the new column, I can see that the dtype is datetime64. I can also see that my dates have been slightly rearranged so that they fit the default order datetime objects (year-month-day)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dates are parsed correctly, we can interact with them in useful ways.\n",
    "\n",
    "What if I run into an error with multiple date formats? While we're specifying the date format here, sometimes you'll run into an error when there are multiple date formats in a single column. If that happens, you have have pandas try to infer what the right date format should be. You can do that like so:\n",
    "\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)\n",
    "\n",
    "Why don't you always use infer_datetime_format = True? \n",
    "There are two big reasons not to always have pandas guess the time format.\n",
    "The first is that pandas won't always been able to figure out the correct date format, especially if someone has gotten creative with data entry.\n",
    "The second is that it's much slower than specifying the exact format of the dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compulsory Tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at all the unique values in the \"Province\" column. \n",
    "# Then convert the column to lowercase and remove any trailing white spaces\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like 'kuram agency' and 'kurram agency' should\n",
    "# be the same city. Correct the dataframe so that they are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PakistanTask.csv file from this dataset and tidying up any \n",
    "# inconsistent columns [city & Province] in that data file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column, date_parsed, in the PakistanTask.csv\n",
    "# dataset that has correctly parsed dates in it. (Don't forget to \n",
    "# double-check that the dtype is correct!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great! Now imagine that we use visualization to discover more areas for cleaning and sanitization. In the next task we will do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is based on material from the following sources:\n",
    "1. Agarwal, M. (n.d.). Pythonic Data Cleaning With NumPy and Pandas. Retrieved April 23, 2019, from Real Python: https://realpython.com/python-data-cleaning-numpy-pandas/\n",
    "2. Kaggle. (2019). Data Cleaning Challenge: Inconsistent Data Entry 5. Retrieved from kaggle: https://www.kaggle.com/blaine12100/data-cleaning-challenge-inconsistent-data-entry-5/data\n",
    "3. Sethi, N. (2018, August 30). Data Cleaning: Parsing Dates. Retrieved from Data Driven Investor: https://medium.com/datadriveninvestor/data-cleaning-parsing-dates-34792fc4d6c8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
